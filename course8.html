<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
</head>
<body>
<h2 id="tidy-data">TIDY DATA</h2>
<p>First read the CSV and set na.strings=c(&quot;NA&quot;,&quot;#DIV/0!&quot;,&quot;&quot;) to make cleaning more simple. Removed column names with at least 80% NA (and null and 'division by 0' combined) and also removed column names without &quot;accel&quot; just as described in the lab instructions.</p>
<pre><code>train&lt;-read.csv(file=&quot;pml-training.csv&quot;,header=TRUE,sep=&quot;,&quot;,na.strings=c(&quot;NA&quot;,&quot;#DIV/0!&quot;,&quot;&quot;))
test&lt;-read.csv(file=&quot;pml-testing.csv&quot;,header=TRUE,sep=&quot;,&quot;,na.strings=c(&quot;NA&quot;,&quot;#DIV/0!&quot;,&quot;&quot;))
mynames&lt;-intersect(names(train[,colSums(is.na(train))/nrow(train)&lt;0.8]),names(train))
mynames&lt;-mynames[grepl(&quot;accel&quot;,mynames)]
train&lt;-train[,c(mynames,&quot;classe&quot;)]
test&lt;-test[,mynames]</code></pre>
<h2 id="partition-data">PARTITION DATA</h2>
<p>Partitioned 60% of data for training, 20% for testing and 20% for validation.</p>
<pre><code>inTrain&lt;-createDataPartition(y=train$classe,p=.6,list=F)
training&lt;-train[inTrain,]
nonTraining&lt;-train[-inTrain,]
inTest&lt;-createDataPartition(y=nonTraining$classe,p=.5,list=F)
testing&lt;-nonTraining[inTest,]
validating&lt;-nonTraining[-inTest,]</code></pre>
<h2 id="train-models">TRAIN MODELS</h2>
<p>Trained with Random Forest (RF) and Gradient Boosting (GB). I think non-linear methods are strongest in such case as for this lab, which is the reason why I picked these two methods. Because of curiousity I also picked Linear Discriminant Analysis. My expectation was to get 70% accuracy with RF and GB and 50% with LDA. A reason for believing in such low accuracy, was that I threw away so many factors according to the instructions.</p>
<h2 id="prediction">PREDICTION</h2>
<p>Predicted and viewed confusion matrices of RF, GB and LDA. The predictions were compared with the testing set. It showed the results were 94%, 83% and 52% for respective method. RF was much above my expectations.</p>
<pre><code>pred_rf&lt;-predict(mod_rf,testing)
pred_gbm&lt;-predict(mod_gbm,testing)
pred_lda&lt;-predict(mod_lda,testing)
confusionMatrix(pred_rf,testing$classe)$overall

##       Accuracy          Kappa  AccuracyLower  AccuracyUpper   AccuracyNull 
##   0.9421361203   0.9268047385   0.9343667992   0.9492383388   0.2844761662 
## AccuracyPValue  McnemarPValue 
##   0.0000000000   0.0004292635

confusionMatrix(pred_gbm,testing$classe)$overall

##       Accuracy          Kappa  AccuracyLower  AccuracyUpper   AccuracyNull 
##   8.251338e-01   7.784878e-01   8.128790e-01   8.369023e-01   2.844762e-01 
## AccuracyPValue  McnemarPValue 
##   0.000000e+00   1.685778e-15

confusionMatrix(pred_lda,testing$classe)$overall

##       Accuracy          Kappa  AccuracyLower  AccuracyUpper   AccuracyNull 
##   5.197553e-01   3.844292e-01   5.039840e-01   5.354971e-01   2.844762e-01 
## AccuracyPValue  McnemarPValue 
##  6.696869e-210  5.161408e-114</code></pre>
<h2 id="stacking">STACKING</h2>
<p>I believed that stacking would improve my method by another 1-2% in the validation set, but I could not see any improvment by stacking further with RF, GAM, GB or LDA, so I continued to use the original RF method to pass the quiz.</p>
<pre><code>confusionMatrix(pred_comb,validating$classe)$overall

##       Accuracy          Kappa  AccuracyLower  AccuracyUpper   AccuracyNull 
##   0.9421361203   0.9268047385   0.9343667992   0.9492383388   0.2844761662 
## AccuracyPValue  McnemarPValue 
##   0.0000000000   0.0004292635

pred_quiz&lt;-predict(mod_rf,test)</code></pre>
</body>
</html>
